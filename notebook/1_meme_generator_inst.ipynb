{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🖼️ Meme Generator Lab: Student Lab\n",
    "## Getting Started\n",
    "Ref repository: https://github.com/IFML-UT/MLLAcademy-2025\n",
    "\n",
    "**What we're going to do inside of this notebook:**\n",
    "\n",
    "> This notebook is used to simulate student interactions with the meme generator pipeline.\n",
    "Use it to validate that the full captioning stack is working before you begin creating a meme of your own.\n",
    "\n",
    "## 🗺️ Roadmap for the entire lab:\n",
    "> The lab is broken down into 3 major sections:\n",
    "\n",
    "1. **Working with LLMs & Inference**: You'll use an open source large language model (LLM) to generate a meme based on general topics or themes. You'll select the best caption from 3 results. We'll be using Meta's `Llama` model family for this text generation\n",
    "\n",
    "\n",
    "2. **Multi-modal Generative AI:** With your caption from the first part of the lab, you will use OpenCLIP to query the top 3 matches from a library of popular meme images to select the best image, based on your text caption. You'll understand what multi-modal means, and how a pre-trained vision transformer model like `ViT-B-32` can return relevant images based on text inputs.\n",
    "\n",
    "3. **Combine both the generated text and best image into your final AI-meme for your finished product.**\n",
    "\n",
    "\n",
    "### ⚙️ How It Works:\n",
    "- Inputs a freeform meme idea or phrase\n",
    "- Classifies it into a pre-approved topic\n",
    "- Uses LLaMA 3.1 8B Instruct (via Hugging Face) to generate 3 clean meme captions\n",
    "- Filters for profanity or off-topic content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup -- Run this cell first to install dependencies and import necessary modules.\n",
    "# This script is designed to be run in a Jupyter notebook environment, such as Google Colab or a local Jupyter setup.\n",
    "# It installs required packages and imports functions for generating safe captions from images.\n",
    "\n",
    "!git clone https://github.com/IFML-UT/MLLAcademy-2025.git\n",
    "# 🛠️ Install dependencies (for Colab or Drive-mount workflows)\n",
    "\n",
    "# --- this cell will create a folder called MLLAcademy-2025 in your current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Requirements & Configure Hugging Face Inference API Token: \n",
    "To help keep this lab computationally light and flexible for our lab use, we are using Hugging Face inference token (generated by IFML) for your use during this week. \n",
    "- API stands for application programing interface, once configured it allows two different software applications communicate and send data to one another. \n",
    "- This secret token will expire after this week. If you would like to continue to run this lab later on your own, you can do so by creating a free HuggingFace account, creating a token within the free tier (https://huggingface.co/settings/tokens) and then pasting your new token into the cell's `getpass` feature below. \n",
    "\n",
    "> “Paste your Hugging Face API token (provided to you) in the cell below when prompted. If you don't have one because you are trying this lab outside of our scheduled session no worries! Visit https://huggingface.co/settings/tokens to create a free account, create a token of `type = READ`, and then copy your access token.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "✅ Hugging Face token found at ../hf_token.txt\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Hugging Face token found at ../hf_token.txt\n",
      "✅ Hugging Face token found at ../hf_token.txt\n",
      "Using LLM mode: text-generation with model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Loading embedding model for semantic topic matching...\n",
      "✅ Hugging Face token found at ../hf_token.txt\n",
      "Using LLM mode: text-generation with model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Loading embedding model for semantic topic matching...\n",
      "\n",
      "Safe Caption Generator module imported successfully, ready to use!\n",
      "\n",
      "Safe Caption Generator module imported successfully, ready to use!\n"
     ]
    }
   ],
   "source": [
    "# This script auto-detects your environment (Colab or local) and configures everything accordingly.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "# --- Detect Environment ---\n",
    "def get_runtime_env():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return \"colab\"\n",
    "    except ImportError:\n",
    "        return \"local\"\n",
    "\n",
    "env = get_runtime_env()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# --- Install Dependencies ---\n",
    "if env == \"colab\":\n",
    "    %pip install -r MLLAcademy-2025/requirements.txt\n",
    "else:\n",
    "    %pip install -r requirements.txt\n",
    "\n",
    "# --- Hugging Face Token Management ---\n",
    "token_path = Path(\"/content/hf_token.txt\") if env == \"colab\" else Path(\"../hf_token.txt\")\n",
    "\n",
    "if not token_path.exists():\n",
    "    print(\"Please enter your Hugging Face API token:\")\n",
    "    token = getpass(\"Hugging Face Token: \")\n",
    "    with open(token_path, \"w\") as f:\n",
    "        f.write(token.strip())\n",
    "    print(f\"✅ Hugging Face token saved to {token_path}\")\n",
    "else:\n",
    "    print(f\"✅ Hugging Face token found at {token_path}\")\n",
    "\n",
    "# --- Ensure utils folder is in sys.path ---\n",
    "sys.path.append(str(Path(\"/content/MLLAcademy-2025/utils\").resolve()) if env == \"colab\" else str(Path(\"../utils\").resolve()))\n",
    "\n",
    "# --- Import the Safe Caption Generator ---\n",
    "from safe_caption_generator import safe_caption_generator\n",
    "print(\"\\nSafe Caption Generator module imported successfully, ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function - for printing captions cleanly and export the results to a JSON file\n",
    "# This file will be used later when we generate the images \n",
    "\n",
    "def print_captions(captions):\n",
    "    env = get_runtime_env()\n",
    "    captions_path = Path(\"/content/MLLAcademy-2025/captions.json\") if env == \"colab\" else Path(\"../captions.json\")\n",
    "\n",
    "    with open(captions_path, \"w\") as f:\n",
    "        json.dump(captions, f)\n",
    "\n",
    "    print(f\"✅ Captions saved to {captions_path}\")\n",
    "    print(\"\\n---\\n\\n\")\n",
    "    for i, c in enumerate(captions, 1):\n",
    "        print(f\"Caption {i}: {c}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type in a prompt between the quotes \n",
    "This will assign your topic to the variable `user_input`\n",
    " - Running the cell below will then run the caption generator and print the captions\n",
    " \n",
    "Additionally, we are going to be using a Python function called `safe_caption_generator` to assist us in prompting the LLM. For example, this code is within the function and prompts the LLM prior to its text generation, based on your input: \n",
    "\n",
    "```\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Write a short, funny meme caption about this topic: {user_input}.\\n\"\n",
    "    \"Only return a single caption, in quotes, with no explanation or extra text.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### We are going to specifically guide our text generation to stay aligned on certain topics.\n",
    "You may find that certain topics will be blocked from use. If you run into a \"try again error message\" please adjust your input. Here are the broad topics we are going to use within this lab for your captions: \n",
    "- \"final exams\"\n",
    "- \"group projects\"\n",
    "- \"studying late\", \n",
    "- \"Monday mornings\"\n",
    "- \"school cafeteria food\"\n",
    "- \"summer break\"\n",
    "- \"forgetting your homework\"\n",
    "- \"getting a pop quiz\"\n",
    "- \"trying to stay awake in class\"\n",
    "- \"sports\"\n",
    "- \"coding projects\"\n",
    "- \"hackathons\"\n",
    "- \"hanging out with friends\"\n",
    "- \"summer weather\"\n",
    "- \"family vacations\"\n",
    "- \"college applications\"\n",
    "-  \"video games\"\n",
    "\n",
    "_You don't have to use these exact words in your `user_input`, but it needs to be semantically similar. For example, \"Going to a baseball game instead of studying\" would match our themes of both `sports` and `forgetting your homework`, and possibly even `studying late`._\n",
    "\n",
    " > Note: This cell may take anywhere from 30 seconds to 2 minutes depending on your prompt and notebook compute resources at the time of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prompt: 'studying all night and finally passing the exam'\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct (Request ID: Root=1-683a8bd1-66bc2a653183a14476db48d0;2455ba22-1ead-4352-832f-8ffc565996e4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:286\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/requests/models.py:1021\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTesting prompt: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Generate and save 3 meme captions and print each\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     captions = \u001b[43msafe_caption_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_captions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     print_captions(captions)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Lab1_MemeGen/utils/safe_caption_generator.py:166\u001b[39m, in \u001b[36msafe_caption_generator\u001b[39m\u001b[34m(user_input, num_captions)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m topic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYour input didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match any approved topics. Try again or rephrase.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m captions = \u001b[43mllama_caption_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_captions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_captions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m filtered = []\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m captions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Lab1_MemeGen/utils/safe_caption_generator.py:103\u001b[39m, in \u001b[36mllama_caption_generator\u001b[39m\u001b[34m(user_input, num_captions)\u001b[39m\n\u001b[32m    101\u001b[39m     output = response.choices[\u001b[32m0\u001b[39m].message[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     response = \u001b[43mllama_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     output = response.strip()\n\u001b[32m    112\u001b[39m quoted = re.findall(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(.*?)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1535\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[39m\n\u001b[32m   1514\u001b[39m         _set_as_non_tgi(model)\n\u001b[32m   1515\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1516\u001b[39m             prompt=prompt,\n\u001b[32m   1517\u001b[39m             details=details,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1533\u001b[39m             decoder_input_details=decoder_input_details,\n\u001b[32m   1534\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1535\u001b[39m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_text_generation.py:526\u001b[39m, in \u001b[36mraise_text_generation_error\u001b[39m\u001b[34m(http_error)\u001b[39m\n\u001b[32m    524\u001b[39m     error_type = payload.get(\u001b[33m\"\u001b[39m\u001b[33merror_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# no payload\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m http_error\n\u001b[32m    528\u001b[39m \u001b[38;5;66;03m# If error_type => more information than `hf_raise_for_status`\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1511\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[39m\n\u001b[32m   1509\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     bytes_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1512\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, BadRequestError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:240\u001b[39m, in \u001b[36mInferenceClient.post\u001b[39m\u001b[34m(self, json, data, model, task, stream)\u001b[39m\n\u001b[32m    237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:333\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response=response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response=response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct (Request ID: Root=1-683a8bd1-66bc2a653183a14476db48d0;2455ba22-1ead-4352-832f-8ffc565996e4)"
     ]
    }
   ],
   "source": [
    "# --- Now we are going to run the safe caption generator based on your input ---\n",
    "# In this cell, we'll test our `safe_caption_generator` function with a sample input. It will:\n",
    "#   - Use your input prompt.\n",
    "#   - Check if the input matches approved topics.\n",
    "#   - Generate 3 captions using a language model.\n",
    "#   - Save the captions to a JSON file for use in later cells.\n",
    "\n",
    "\n",
    "try:\n",
    "    # modify this input to test different prompts \n",
    "    user_input = \"studying all night and finally passing the exam\"\n",
    "    print(f\"Testing prompt: '{user_input}'\")\n",
    "    \n",
    "    # Generate and save 3 meme captions and print each\n",
    "    captions = safe_caption_generator(user_input, num_captions=3)\n",
    "    print_captions(captions)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"⚠️ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out!\n",
    "Use the box below to enter your meme idea, click \"Generate,\" and see three captions!\n",
    "\n",
    "Each generation is saved in `captions.json` for use in the next part of the lab. Each new generation overwrites that file. If you want to save any specific caption, save it in a new file within your directory. You'll have a chance to select your favorite caption in the next lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcda119cedc344898e445a86ef8464e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', placeholder='Enter your meme idea...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481d8b3a018c4f40901dd5e6cf130770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ae5f7098d54fb3aad2bec8e29d1bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive Prompt (for Demo in class)\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "input_box = widgets.Text(value='', placeholder='Enter your meme idea...', description='Prompt:')\n",
    "run_button = widgets.Button(description=\"Generate\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def run_on_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        try:\n",
    "            captions = safe_caption_generator(input_box.value)\n",
    "            # for idx, c in enumerate(captions, 1): # backup code to print each caption rather than use function\n",
    "            #   print(f\"{idx}. {c}\")\n",
    "            print_captions(captions)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error: {e}\")\n",
    "\n",
    "run_button.on_click(run_on_click)\n",
    "display(input_box, run_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "- If you get a profanity or topic error, verify the input is:\n",
    "  - Clean (no banned phrases)\n",
    "  - Topically close to: studying, group projects, sports, coding, school, etc.\n",
    "\n",
    "- If you get an API error:\n",
    "  - Ensure `hf_token.txt` exists and contains a valid Hugging Face token; if the token is missing, please ask for a new token.\n",
    "  - Ensure `.gitignore` excludes it from version control\n",
    "\n",
    "- If you get no captions back:\n",
    "  - Check output formatting with `print(repr(captions))`\n",
    "  - Rerun cell — model output may vary by seed\n",
    "\n",
    "---\n",
    "✅ Instructor notebook complete. Move on to Notebook A when you're ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
