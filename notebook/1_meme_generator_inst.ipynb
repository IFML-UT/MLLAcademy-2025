{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🖼️ Meme Generator Lab: Student Lab\n",
    "## Getting Started\n",
    "Ref repository: https://github.com/IFML-UT/MLLAcademy-2025\n",
    "\n",
    "**What we're going to do inside of this notebook:**\n",
    "\n",
    "This notebook is used to simulate and guide your interactions with the meme generator pipeline.\n",
    "We're going to walk through each step behind generating a meme - why a meme?\n",
    "\n",
    "- We're going to use natural language to generate new text on a topic of your choosing\n",
    "- We'll then take this generated caption and find the best image match for the text - this \"text to image\" type of generative AI is referred to as multi-modal. \n",
    "- Lastly, you'll use python libraries within this lab to bring the text and the image together to create a unique meme. \n",
    "\n",
    "## 🗺️ Roadmap for the entire lab:\n",
    "> The lab is broken down into 3 major sections:\n",
    "\n",
    "1. **Working with LLMs & Inference**: You'll use an open source large language model (LLM) to generate a meme based on general topics or themes. You'll select the best caption from 3 results. We'll be using Meta's `Llama` model family for this text generation, and can experiment with others as well. \n",
    "\n",
    "\n",
    "2. **Multi-modal Generative AI:** With your caption from the first part of the lab, you will use OpenCLIP to query the top 3 matches from a library of popular meme images to select the best image, based on your text caption. You'll understand what multi-modal means, and how a pre-trained vision transformer model like `ViT-B-32` can return relevant images based on text inputs.\n",
    "\n",
    "3. **Combine both the generated text and best image into your final AI-meme for your finished product.**\n",
    "\n",
    "\n",
    "### ⚙️ How It Works:\n",
    "- Inputs a freeform meme idea or phrase\n",
    "- Classifies it into a pre-approved topic\n",
    "- Uses LLaMA 3.1 8B Instruct (via Hugging Face) to generate 3 clean meme captions\n",
    "- Filters for profanity or off-topic content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup -- Run this cell first to install dependencies and import necessary modules.\n",
    "# This script is designed to be run in a Jupyter notebook environment, such as Google Colab or a local Jupyter setup.\n",
    "# It installs required packages and imports functions for generating safe captions from images.\n",
    "# --- this cell will create a folder called MLLAcademy-2025 in your current directory\n",
    "\n",
    "!git clone https://github.com/IFML-UT/MLLAcademy-2025.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Requirements & Configure Hugging Face Inference API Token: \n",
    "To help keep this lab computationally light and flexible for our lab use, we are using Hugging Face inference token (generated by IFML) for your use during this week. \n",
    "- API stands for application programing interface, once configured it allows two different software applications communicate and send data to one another. \n",
    "- This secret token will expire after this week. \n",
    "- If you would like to continue to run this lab later on your own, you can do so by creating a free HuggingFace account, creating a token within the free tier (https://huggingface.co/settings/tokens) and then pasting your new token into the cell's `getpass` feature below. \n",
    "\n",
    "Paste your Hugging Face API token (provided to you) in the cell below when prompted. \n",
    "\n",
    "> If you don't have one because you are trying this lab outside of our scheduled session no worries! \n",
    "> Visit https://huggingface.co/settings/tokens to create a free account, create a token of `type = READ`, and then copy your access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "✅ Hugging Face token found at ../hf_token.txt\n",
      "🧭 Detected environment: local\n",
      "✅ Hugging Face token found at ../hf_token.txt\n",
      "Loading embedding model for semantic topic matching...\n",
      "\n",
      "Safe Caption Generator module imported successfully, ready to use!\n"
     ]
    }
   ],
   "source": [
    "# This script auto-detects your environment (Colab or local) and configures everything accordingly.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "# --- Detect Environment ---\n",
    "def get_runtime_env():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return \"colab\"\n",
    "    except ImportError:\n",
    "        return \"local\"\n",
    "\n",
    "env = get_runtime_env()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# --- Install Dependencies ---\n",
    "if env == \"colab\":\n",
    "    %pip install -r MLLAcademy-2025/requirements.txt\n",
    "else:\n",
    "    %pip install -r requirements.txt\n",
    "\n",
    "# --- Hugging Face Token Management ---\n",
    "token_path = Path(\"/content/hf_token.txt\") if env == \"colab\" else Path(\"../hf_token.txt\")\n",
    "\n",
    "if not token_path.exists():\n",
    "    print(\"Please enter your Hugging Face API token:\")\n",
    "    token = getpass(\"Hugging Face Token: \")\n",
    "    with open(token_path, \"w\") as f:\n",
    "        f.write(token.strip())\n",
    "    print(f\"✅ Hugging Face token saved to {token_path}\")\n",
    "else:\n",
    "    print(f\"✅ Hugging Face token found at {token_path}\")\n",
    "\n",
    "# --- Ensure utils folder is in sys.path ---\n",
    "sys.path.append(str(Path(\"/content/MLLAcademy-2025/utils\").resolve()) if env == \"colab\" else str(Path(\"../utils\").resolve()))\n",
    "\n",
    "# --- Import the Safe Caption Generator ---\n",
    "from safe_caption_generator import safe_caption_generator\n",
    "print(\"\\nSafe Caption Generator module imported successfully, ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function - for printing captions cleanly and export the results to a JSON file\n",
    "# This file will be used later when we generate the images \n",
    "\n",
    "def print_captions(captions):\n",
    "    env = get_runtime_env()\n",
    "    captions_path = Path(\"/content/MLLAcademy-2025/captions.json\") if env == \"colab\" else Path(\"../captions.json\")\n",
    "\n",
    "    with open(captions_path, \"w\") as f:\n",
    "        json.dump(captions, f)\n",
    "\n",
    "    print(f\"✅ Captions saved to {captions_path}\")\n",
    "    print(\"\\n---\\n\\n\")\n",
    "    for i, c in enumerate(captions, 1):\n",
    "        print(f\"Caption {i}: {c}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Type in a prompt between the quotes \n",
    "This will assign your topic to the variable `user_input`\n",
    " - Running the cell below will then run the caption generator and print the captions\n",
    " \n",
    "Additionally, we are going to be using a Python function called `safe_caption_generator` to assist us in prompting the LLM. For example, this code is within the function and prompts the LLM prior to its text generation, based on your input: \n",
    "\n",
    "```\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Write a short, funny meme caption about this topic: {user_input}.\\n\"\n",
    "    \"Only return a single caption, in quotes, with no explanation or extra text.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### We are going to specifically guide our text generation to stay aligned on certain topics.\n",
    "You may find that certain topics will be blocked from use. If you run into a \"try again error message\" please adjust your input. Here are the broad topics we are going to use within this lab for your captions: \n",
    "- \"final exams\"\n",
    "- \"group projects\"\n",
    "- \"studying late\", \n",
    "- \"Monday mornings\"\n",
    "- \"school cafeteria food\"\n",
    "- \"summer break\"\n",
    "- \"forgetting your homework\"\n",
    "- \"getting a pop quiz\"\n",
    "- \"trying to stay awake in class\"\n",
    "- \"sports\"\n",
    "- \"coding projects\"\n",
    "- \"hackathons\"\n",
    "- \"hanging out with friends\"\n",
    "- \"summer weather\"\n",
    "- \"family vacations\"\n",
    "- \"college applications\"\n",
    "-  \"video games\"\n",
    "\n",
    "_You don't have to use these exact words in your `user_input`, but it needs to be semantically similar. For example, \"Going to a baseball game instead of studying\" would match our themes of both `sports` and `forgetting your homework`, and possibly even `studying late`._\n",
    "\n",
    " > Note: This cell may take anywhere from 30 seconds to 2 minutes depending on your prompt and notebook compute resources at the time of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prompt: 'summer vacation can't come soon enough'\n",
      "✅ Captions saved to ../captions.json\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Caption 1: Apparently summer is just a myth perpetuated by palm trees and ice cream trucks to make us survive the bleak 9 months of misery\n",
      "\n",
      "Caption 2: Just remembered,'summer vacation' is just a nice way of saying 'adulting is over' for 3 whole months\n",
      "\n",
      "Caption 3: Summer can't come soon enough... said every student ever\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Now we are going to run the safe caption generator based on your input ---\n",
    "# In this cell, we'll test our `safe_caption_generator` function with a sample input. It will:\n",
    "#   - Use your input prompt.\n",
    "#   - Check if the input matches approved topics.\n",
    "#   - Generate 3 captions using a language model.\n",
    "#   - Save the captions to a JSON file for use in later cells.\n",
    "\n",
    "try:\n",
    "    # modify this input to test different prompts \n",
    "    user_input = \"summer vacation can't come soon enough\"\n",
    "    print(f\"Testing prompt: '{user_input}'\")\n",
    "    \n",
    "    # Generate and save 3 meme captions and print each\n",
    "    captions = safe_caption_generator(user_input, num_captions=3)\n",
    "    print_captions(captions)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"⚠️ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate \"top 3\" captions for your meme\n",
    "Use the box below to enter your meme idea, click \"Generate,\" and see three captions!\n",
    "\n",
    "This specific cell below will save to `captions.json` for use in the next part of the lab. Each new generation overwrites the previous contents of that file. Feel free to use the cell above this one to get a feel for how much (or how little) detail on your topic you want to include in your prompt and observe the quality of the LLM response across the 3 caption options.\n",
    "\n",
    "If you want to save any specific caption, save it in a new file within your directory. You'll have a chance to select your favorite caption in the next lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c504335395a452287797ff2e9409256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', placeholder='Enter your meme idea...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704b4c3b8b9e4e54938de4221daf2f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d895720699467ba05a92bb01cb6445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive Prompt (for Demo in class)\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "input_box = widgets.Text(value='', placeholder='Enter your meme idea...', description='Prompt:')\n",
    "run_button = widgets.Button(description=\"Generate\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def run_on_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        try:\n",
    "            captions = safe_caption_generator(input_box.value)\n",
    "            # for idx, c in enumerate(captions, 1): # backup code to print each caption rather than use function\n",
    "            #   print(f\"{idx}. {c}\")\n",
    "            print_captions(captions)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error: {e}\")\n",
    "\n",
    "run_button.on_click(run_on_click)\n",
    "display(input_box, run_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Troubleshooting Guide\n",
    "\n",
    "- If you get a profanity or topic error, verify the input is:\n",
    "  - Clean (no banned phrases)\n",
    "  - Topically close to: studying, group projects, sports, coding, school, etc.\n",
    "\n",
    "- If you get an API error:\n",
    "  - Ensure `hf_token.txt` exists and contains a valid Hugging Face token; if the token is missing, please ask for a new token.\n",
    "  - Ensure `.gitignore` excludes it from version control\n",
    "\n",
    "- If you get no captions back:\n",
    "  - Check output formatting with `print(repr(captions))`\n",
    "  - Rerun cell — model output may vary by seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps: \n",
    "\n",
    "✅ Instructor notebook complete. You have successfully: \n",
    "\n",
    "1. Invoked an open-source LLM via the Hugging Face API and generated text using a cloud-based inference service. \n",
    "2. Observed how different prompting can result in different text results. \n",
    "3. Generated a set of 3 captions based on a topic that have been saved in your directory as: `captions.json` - find and open that file, and you'll see your three captions. \n",
    "\n",
    "You are now ready to move on to the next notebook: `2_meme_generator_A.ipynb`!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
