{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Meme Generator Lab: Student Lab <> Getting Started\n",
    "Ref repository: https://github.com/IFML-UT/MLLAcademy-2025\n",
    "\n",
    "**What we're going to do inside of this notebook:**\n",
    "\n",
    "> This notebook is used to simulate student interactions with the meme generator pipeline.\n",
    "Use it to validate that the full captioning stack is working before you begin creating a meme of your own.\n",
    "\n",
    "## üó∫Ô∏è Roadmap for the entire lab:\n",
    "> The lab is broken down into 3 major sections:\n",
    "\n",
    "1. **Working with LLMs & Inference**: You'll use an open source large language model (LLM) to generate a meme based on general topics or themes. You'll select the best caption from 3 results. We'll be using Meta's `Llama` model family for this text generation\n",
    "\n",
    "\n",
    "2. **Multi-modal Generative AI:** With your caption from the first part of the lab, you will use OpenCLIP to query the top 3 matches from a library of popular meme images to select the best image, based on your text caption. You'll understand what multi-modal means, and how a pre-trained vision transformer model like `ViT-B-32` can return relevant images based on text inputs.\n",
    "\n",
    "3. **Combine both the generated text and best image into your final AI-meme for your finished product.**\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è How It Works:\n",
    "- Inputs a freeform meme idea or phrase\n",
    "- Classifies it into a pre-approved topic\n",
    "- Uses LLaMA 3.1 8B Instruct (via Hugging Face) to generate 3 clean meme captions\n",
    "- Filters for profanity or off-topic content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup -- Run this cell first to install dependencies and import necessary modules.\n",
    "# This script is designed to be run in a Jupyter notebook environment, such as Google Colab or a local Jupyter setup.\n",
    "# It installs required packages and imports functions for generating safe captions from images.\n",
    "\n",
    "!git clone https://github.com/IFML-UT/MLLAcademy-2025.git\n",
    "# üõ†Ô∏è Install dependencies (for Colab or Drive-mount workflows)\n",
    "\n",
    "# --- this cell will create a folder called MLLAcademy-2025 in your current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Requirements & Configure Hugging Face Inference API Token: \n",
    "To help keep this lab computationally light and flexible for our lab use, we are using Hugging Face inference token (generated by IFML) for your use during this week. \n",
    "- API stands for application programing interface, once configured it allows two different software applications communicate and send data to one another. \n",
    "- This secret token will expire after this week. If you would like to continue to run this lab later on your own, you can do so by creating a free HuggingFace account, creating a token within the free tier (https://huggingface.co/settings/tokens) and then pasting your new token into the cell's `getpass` feature below. \n",
    "\n",
    "> ‚ÄúPaste your Hugging Face API token (provided to you) in the cell below when prompted. If you don't have one because you are trying this lab outside of our scheduled session no worries! Visit https://huggingface.co/settings/tokens to create a free account, create a token of `type = READ`, and then copy your access token.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Cannot install huggingface_hub==0.20.3 and transformers==4.50.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/Users/jwhorley/Library/Python/3.11/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0mLoading embedding model for semantic topic matching...\n"
     ]
    }
   ],
   "source": [
    "# Setup -- Run this cell first to install dependencies and import necessary modules.\n",
    "!pip install -r MLLAcademy-2025/requirements.txt\n",
    "\n",
    "# !pip install -q \\\n",
    "#   torch \\\n",
    "#   transformers==4.50.0 \\\n",
    "#   huggingface_hub==0.20.3 \\\n",
    "#   sentence-transformers==2.2.2 \\\n",
    "#   accelerate==0.21.0 \\\n",
    "#   better_profanity \\\n",
    "#   ipywidgets==7.6.5\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "# --- Populate our hf_token.txt file with the secret token for access to the\n",
    "# --- Hugging Face API. This is needed to access the model and generate captions.\n",
    "\n",
    "token_path = Path(\"../hf_token.txt\")\n",
    "if not token_path.exists():\n",
    "    print(\"Please enter your Hugging Face API token:\")\n",
    "    token = getpass(\"Hugging Face Token: \")\n",
    "    with open(token_path, \"w\") as f:\n",
    "        f.write(token.strip())\n",
    "    print(f\"‚úÖ Hugging Face token saved to {token_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Hugging Face token found at {token_path}\")\n",
    "\n",
    "# Ensure utils folder is in path for import specific for Google Colab or Drive-mount workflows\n",
    "sys.path.append(str(Path(\"/content/MLLAcademy-2025/utils\").resolve()))\n",
    "# Import python file for generation and safe captioning\n",
    "from safe_caption_generator import safe_caption_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function - for printing captions cleanly and export the results to a JSON file\n",
    "# This file will be used later when we generate the images \n",
    "\n",
    "def print_captions(captions):\n",
    "    with open(\"../captions.json\", \"w\") as f:\n",
    "        json.dump(captions, f)\n",
    "    print(\"\\n---\\n\\n\")\n",
    "    for i, c in enumerate(captions, 1):\n",
    "        print(f\"Caption {i}: {c}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type in a prompt between the quotes \n",
    "This will assign your topic to the variable `user_input`\n",
    " - Running the cell below will then run the caption generator and print the captions\n",
    " \n",
    "Additionally, we are going to be using a Python function called `safe_caption_generator` to assist us in prompting the LLM. For example, this code is within the function and prompts the LLM prior to its text generation, based on your input: \n",
    "\n",
    "```\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Write a short, funny meme caption about this topic: {user_input}.\\n\"\n",
    "    \"Only return a single caption, in quotes, with no explanation or extra text.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### We are going to specifically guide our text generation to stay aligned on certain topics.\n",
    "You may find that certain topics will be blocked from use. If you run into a \"try again error message\" please adjust your input. Here are the broad topics we are going to use within this lab for your captions: \n",
    "- \"final exams\"\n",
    "- \"group projects\"\n",
    "- \"studying late\", \n",
    "- \"Monday mornings\"\n",
    "- \"school cafeteria food\"\n",
    "- \"summer break\"\n",
    "- \"forgetting your homework\"\n",
    "- \"getting a pop quiz\"\n",
    "- \"trying to stay awake in class\"\n",
    "- \"sports\"\n",
    "- \"coding projects\"\n",
    "- \"hackathons\"\n",
    "- \"hanging out with friends\"\n",
    "- \"summer weather\"\n",
    "- \"family vacations\"\n",
    "- \"college applications\"\n",
    "-  \"video games\"\n",
    "\n",
    "_You don't have to use these exact words in your `user_input`, but it needs to be semantically similar. For example, \"Going to a baseball game instead of studying\" would match our themes of both `sports` and `forgetting your homework`, and possibly even `studying late`._\n",
    "\n",
    " > Note: This cell may take anywhere from 30 seconds to 2 minutes depending on your prompt and notebook compute resources at the time of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prompt: 'summer school or summer break?'\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Caption 1: Summer school: where you pay to relearn what you forgot, and also get sunburned, but still no homework\n",
      "\n",
      "Caption 2: Better a tan, than a GPA\n",
      "\n",
      "Caption 3: Is summer school like when you burn toast but you're too lazy to make another piece. It's like, why fix it if you can just make do with it?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Now we are going to run the safe caption generator based on your input ---\n",
    "\n",
    "try: \n",
    "    user_input = \"summer school or summer break?\"\n",
    "    print(f\"Testing prompt: '{user_input}'\")\n",
    "    captions = safe_caption_generator(user_input, num_captions=3)\n",
    "    print_captions(captions)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcda119cedc344898e445a86ef8464e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', placeholder='Enter your meme idea...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481d8b3a018c4f40901dd5e6cf130770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ae5f7098d54fb3aad2bec8e29d1bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive Prompt (for Demo in class)\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "input_box = widgets.Text(value='', placeholder='Enter your meme idea...', description='Prompt:')\n",
    "run_button = widgets.Button(description=\"Generate\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def run_on_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        try:\n",
    "            captions = safe_caption_generator(input_box.value)\n",
    "            # for idx, c in enumerate(captions, 1): # backup code to print each caption rather than use function\n",
    "            #   print(f\"{idx}. {c}\")\n",
    "            print_captions(captions)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "\n",
    "run_button.on_click(run_on_click)\n",
    "display(input_box, run_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "- If you get a profanity or topic error, verify the input is:\n",
    "  - Clean (no banned phrases)\n",
    "  - Topically close to: studying, group projects, sports, coding, school, etc.\n",
    "\n",
    "- If you get an API error:\n",
    "  - Ensure `hf_token.txt` exists and contains a valid Hugging Face token; if the token is missing, please ask for a new token.\n",
    "  - Ensure `.gitignore` excludes it from version control\n",
    "\n",
    "- If you get no captions back:\n",
    "  - Check output formatting with `print(repr(captions))`\n",
    "  - Rerun cell ‚Äî model output may vary by seed\n",
    "\n",
    "---\n",
    "‚úÖ Instructor notebook complete. Move on to Notebook A when you're ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
