{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a153e30",
   "metadata": {},
   "source": [
    "# 🖼️ Meme Generator Lab: Student Lab\n",
    "## Getting Started\n",
    "Ref repository: https://github.com/IFML-UT/MLLAcademy-2025\n",
    "\n",
    "**What we're going to do inside of this notebook:**\n",
    "\n",
    "This notebook is used to simulate and guide your interactions with the meme generator pipeline.\n",
    "We're going to walk through each step behind generating a meme - why a meme? --> it's **multi-modal!**\n",
    "\n",
    "- We're going to use **natural language processing (NLP)** to generate new text on a topic of your choosing\n",
    "- We'll then take this generated caption and find the best image match for the text - this \"text to image\" type of generative AI is referred to as multi-modal. \n",
    "- Lastly, you'll use python libraries within this lab to bring the text and the image together to create a unique meme. \n",
    "\n",
    "## 🗺️ Roadmap for the entire lab:\n",
    "> The lab is broken down into 3 major sections:\n",
    "\n",
    "1. **Working with LLMs & Inference**: You'll use an open source large language model (LLM) to generate a meme based on general topics or themes. You'll select the best caption from 3 results. We'll be using Meta's `Llama` model family for this text generation, and can experiment with others as well. \n",
    "\n",
    "\n",
    "2. **Multi-modal Generative AI:** With your caption from the first part of the lab, you will use OpenCLIP to query the top 3 matches from a library of popular meme images to select the best image, based on your text caption. You'll understand what multi-modal means, and how a pre-trained vision transformer model like `ViT-B-32` can return relevant images based on text inputs.\n",
    "\n",
    "3. **Combine both the generated text and best image into your final AI-meme for your finished product.** You'll have the option of sharing your favorite meme with the class by uploading it to our shared drive for the week. \n",
    "\n",
    "\n",
    "### ⚙️ How It Works:\n",
    "- Inputs a freeform meme idea or phrase\n",
    "- Classifies it into a pre-approved topic\n",
    "- Uses LLaMA 3.1 8B Instruct (via Hugging Face) to generate 3 clean meme captions\n",
    "- Filters for profanity or off-topic content\n",
    "\n",
    "### For use later (after this lab) on your own: \n",
    "If you would like to clone this repo to your own computer, and run it later you can use the following command to copy it to your machine: \n",
    "\n",
    "\n",
    "-- Run this cell first to install dependencies and import necessary modules.\n",
    "- This `git clone` command will create a folder called MLLAcademy-2025 in your current directory. \n",
    "- Note: you'll need to supply your own API tokens for access to any paid models, or to Hugging Face for any open-source hosted model.\n",
    "\n",
    "> `!git clone https://github.com/IFML-UT/MLLAcademy-2025.git`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de5637",
   "metadata": {},
   "source": [
    "## 1️⃣ Load Requirements & Configure Hugging Face Inference API Token: \n",
    "To help keep this lab computationally light and flexible for our lab use, we are using Hugging Face inference token (generated by IFML) for your use during this week. \n",
    "- API stands for application programing interface, once configured it allows two different software applications communicate and send data to one another. \n",
    "- This secret token will expire after this week. \n",
    "- If you would like to continue to run this lab later on your own, you can do so by creating a free HuggingFace account, creating a token within the free tier (https://huggingface.co/settings/tokens) and then pasting your new token into the cell's `getpass` feature below. \n",
    "\n",
    "Paste your Hugging Face API token (provided to you) in the cell below when prompted. \n",
    "\n",
    "> If you don't have one because you are trying this lab outside of our scheduled session no worries! \n",
    "> Visit https://huggingface.co/settings/tokens to create a free account, create a token of `type = READ`, and then copy your access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo to this notebook's runtime:\n",
    "import os\n",
    "git clone https://github.com/IFML-UT/MLLAcademy-2025.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script auto-detects your environment (Colab or local) and configures everything accordingly.\n",
    "# --- We're going to run this in Google Colab today ---\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "# --- Detect Environment ---\n",
    "def get_runtime_env():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return \"colab\"\n",
    "    except ImportError:\n",
    "        return \"local\"\n",
    "\n",
    "env = get_runtime_env()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# --- Install Dependencies ---\n",
    "if env == \"colab\":\n",
    "    %pip install -r /content/MLLAcademy-2025/requirements.txt\n",
    "else:\n",
    "    req_path = str(Path(\"../requirements.txt\").resolve())\n",
    "    %pip install -r $req_path\n",
    "\n",
    "# --- Hugging Face Token Management ---\n",
    "token_path = Path(\"/content/hf_token.txt\") if env == \"colab\" else Path(\"../hf_token.txt\")\n",
    "\n",
    "if not token_path.exists():\n",
    "    print(\"Please enter your Hugging Face API token:\")\n",
    "    token = getpass(\"Hugging Face Token: \")\n",
    "    with open(token_path, \"w\") as f:\n",
    "        f.write(token.strip())\n",
    "    print(f\"✅ Hugging Face token saved to {token_path}\")\n",
    "else:\n",
    "    print(f\"✅ Hugging Face token found at {token_path}\")\n",
    "\n",
    "# --- Ensure utils folder is in sys.path ---\n",
    "sys.path.append(str(Path(\"/content/MLLAcademy-2025/utils\").resolve()) if env == \"colab\" else str(Path(\"../utils\").resolve()))\n",
    "\n",
    "# --- Import the Safe Caption Generator ---\n",
    "from safe_caption_generator import safe_caption_generator\n",
    "print(\"\\nSafe Caption Generator module imported successfully, ready to use!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4d325",
   "metadata": {},
   "source": [
    "#### After running the cell above, you should see the following three statements print to the notebook:\n",
    "\n",
    "> `✅ Hugging Face token found at ../hf_token.txt`<br>\n",
    "> `Loading embedding model for semantic topic matching...`\n",
    "\n",
    "> `Safe Caption Generator module imported successfully, ready to use!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675961d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function - for printing captions cleanly and export the results to a JSON file\n",
    "# This file will be used later when we generate the images \n",
    "# --- Run this cell ---\n",
    "\n",
    "def print_captions(captions):\n",
    "    env = get_runtime_env()\n",
    "    captions_path = Path(\"/content/MLLAcademy-2025/captions.json\") if env == \"colab\" else Path(\"../captions.json\")\n",
    "\n",
    "    with open(captions_path, \"w\") as f:\n",
    "        json.dump(captions, f)\n",
    "\n",
    "    print(f\"✅ Captions saved to {captions_path}\")\n",
    "    print(\"\\n---\\n\\n\")\n",
    "    for i, c in enumerate(captions, 1):\n",
    "        print(f\"Caption {i}: {c}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8690f9",
   "metadata": {},
   "source": [
    "## 2️⃣ Tests Different Prompts:\n",
    "This will assign your topic to the variable `user_input`\n",
    " - Running the cell below will then run the caption generator and print the captions\n",
    " \n",
    "Additionally, we are going to be using a Python function called `safe_caption_generator` to assist us in prompting the LLM. For example, this code is within the function and prompts the LLM prior to its text generation, based on your input: \n",
    "\n",
    "```\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Write a short, funny meme caption about this topic: {user_input}.\\n\"\n",
    "    \"Only return a single caption, in quotes, with no explanation or extra text.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### We are going to specifically guide our text generation to stay aligned on certain topics.\n",
    "You may find that certain topics will be blocked from use. If you run into a \"try again error message\" please adjust your input. Here are the broad topics we are going to use within this lab for your captions: \n",
    "- \"final exams\"\n",
    "- \"group projects\"\n",
    "- \"studying late\", \n",
    "- \"Monday mornings\"\n",
    "- \"school cafeteria food\"\n",
    "- \"summer break\"\n",
    "- \"forgetting your homework\"\n",
    "- \"getting a pop quiz\"\n",
    "- \"trying to stay awake in class\"\n",
    "- \"sports\"\n",
    "- \"coding projects\"\n",
    "- \"hackathons\"\n",
    "- \"hanging out with friends\"\n",
    "- \"summer weather\"\n",
    "- \"family vacations\"\n",
    "- \"college applications\"\n",
    "-  \"video games\"\n",
    "\n",
    "_You don't have to use these exact words in your `user_input`, but it needs to be semantically similar. For example, \"Going to a baseball game instead of studying\" would match our themes of both `sports` and `forgetting your homework`, and possibly even `studying late`._\n",
    "\n",
    " > Note: This cell may take anywhere from 30 seconds to 2 minutes depending on your prompt and notebook compute resources at the time of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Now we are going to run the safe caption generator based on your input ---\n",
    "# In this cell, we'll test our `safe_caption_generator` function with a sample input. It will:\n",
    "#   - Use your input prompt.\n",
    "#   - Check if the input matches approved topics.\n",
    "#   - Generate 3 captions using a language model.\n",
    "#   - Save the captions to a JSON file for use in later cells.\n",
    "\n",
    "try:\n",
    "    # modify this input to test different prompts \n",
    "    user_input = \"your prompt theme or topic here\"\n",
    "    print(f\"Testing prompt: '{user_input}'\")\n",
    "    \n",
    "    # Generate and save 3 meme captions and print each\n",
    "    captions = safe_caption_generator(user_input, num_captions=3)\n",
    "    print_captions(captions)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"⚠️ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99204f4",
   "metadata": {},
   "source": [
    "## 3️⃣ Generate \"top 3\" captions for your meme\n",
    "Use the box below to enter your meme idea, click \"Generate,\" and see three captions!\n",
    "\n",
    "**This specific cell below will save to `captions.json` for use in the next part of the lab.** \n",
    "\n",
    "Each new generation overwrites the previous contents of that file. Feel free to use the cell above this one to get a feel for how much (or how little) detail on your topic you want to include in your prompt and observe the quality of the LLM response across the 3 caption options.\n",
    "\n",
    "If you want to save any specific caption, save it in a new file within your directory. You'll have a chance to select your favorite caption in the next lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Prompt & Demo:\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "input_box = widgets.Text(value='', placeholder='Enter your meme idea...', description='Prompt:')\n",
    "run_button = widgets.Button(description=\"Generate\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def run_on_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        try:\n",
    "            captions = safe_caption_generator(input_box.value)\n",
    "            # for idx, c in enumerate(captions, 1): # backup code to print each caption rather than use function\n",
    "            #   print(f\"{idx}. {c}\")\n",
    "            print_captions(captions)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error: {e}\")\n",
    "\n",
    "run_button.on_click(run_on_click)\n",
    "display(input_box, run_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd8f5e",
   "metadata": {},
   "source": [
    "## 4️⃣ Troubleshooting Guide\n",
    "\n",
    "- If you get a profanity or topic error, verify the input is:\n",
    "  - Clean (no banned phrases)\n",
    "  - Topically close to: studying, group projects, sports, coding, school, etc.\n",
    "\n",
    "- If you get an API error:\n",
    "  - Ensure `hf_token.txt` exists and contains a valid Hugging Face token; if the token is missing, please ask for a new token.\n",
    "  - Ensure `.gitignore` excludes it from version control\n",
    "\n",
    "- If you get no captions back:\n",
    "  - Check output formatting with `print(repr(captions))`\n",
    "  - Rerun cell — model output may vary by seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503ff74",
   "metadata": {},
   "source": [
    "## 5️⃣ Checkpoint Complete! \n",
    "\n",
    "✅ LLM text generation complete! You have successfully: \n",
    "\n",
    "1. Invoked an open-source LLM via the Hugging Face API and generated text using a cloud-based inference service. \n",
    "2. Observed how different prompting can result in different text results. \n",
    "3. Generated a set of 3 captions based on a topic that have been saved in your directory as: `captions.json` - find and open that file, and you'll see your three captions. \n",
    "\n",
    "## Next Steps:\n",
    "#### Continue to the image generation portion of the lab below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30322215",
   "metadata": {},
   "source": [
    "# 6️⃣ Meme Image Selector with OpenCLIP 🖼️\n",
    "\n",
    "In this portion of the notebook, you'll take your meme **captions** from the previous notebook, select one of them, and use it to semantically search a pool of 35 preloaded images.\n",
    "\n",
    "You'll use OpenCLIP (an open-source model trained to match images to text) to:\n",
    "- Embed your caption\n",
    "- Compare it to image embeddings\n",
    "- Rank the most relevant images\n",
    "- Visualize the top 3 results\n",
    "\n",
    "> Learn more about OpenCLIP here: https://github.com/mlfoundations/open_clip\n",
    "\"CLIP\" stands for Contrastive Language-Image Pre-training, and OpenCLIP is an open-source alternative\n",
    "to OpenAI's CLIP model. \n",
    "\n",
    "This notebook sets you up to later combine the image + caption into a final meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad49d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "# 🛠️ Install required libraries\n",
    "# These libraries are required for the code to run. We are going to use \"pip\" to install them.\n",
    "\n",
    "!pip install -q \\\n",
    "  open_clip_torch \\\n",
    "  torchvision \\\n",
    "  ftfy \\\n",
    "  regex \\\n",
    "  tqdm \\\n",
    "  matplotlib\n",
    "\n",
    "# Imports and Model Setup\n",
    "# Now we are going to import the required libraries and set up the model.\n",
    "# We are going to use the \"open_clip\" library to load the CLIP model.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa2c28",
   "metadata": {},
   "source": [
    "## 7️⃣ Load OpenCLIP: model (ViT-B-32)\n",
    "This model has been trained on a dataset of **34 billion** image<>caption pairs, and has a 72.8% zero-shot accuracy. \n",
    "\n",
    "In the cell below you'll load and assign the model to variable `model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenCLIP model (ViT-B-32 for speed, pre-trained on LAION-2B)\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be59ea",
   "metadata": {},
   "source": [
    "## 🎱 Getting started with image search: 🔎\n",
    "Now we are going to load the images that we want to use for the lab.\n",
    " - We are going to use the \"Path\" library to load the images from the directory.\n",
    "\n",
    "The \"glob\" method to load all the images from the directory, and we're using the \"assert\" method to check if the images are loaded correctly - this is an example of error handling within Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d383bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use the \"Path\" library to load the images from the directory.\n",
    "\n",
    "data_dir = Path(\"/content/MLLAcademy-2025/images\")  # 35 preloaded images for lab\n",
    "image_paths = list(data_dir.glob(\"*.jpg\")) + list(data_dir.glob(\"*.png\"))\n",
    "assert len(image_paths) >= 1, \"No images found in image directory.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d094624",
   "metadata": {},
   "source": [
    "## 9️⃣ Create a function to view the selected images in the notebook\n",
    "\n",
    "For this task, we'll use a popular Python libray in both machine learning and data science: `matplotlib`. \n",
    "You can think of it as a set of tools that allow you to visualize and create graphs and images of data. In this case, our data are the images returned by our `model`. \n",
    "\n",
    "We're using  `f string` to format (and later print) the `score` of each image returned. This score represents the model's confidence that image is a good match, based on the input. This is commonly coded as: `f\"string {expression}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run this cell ---\n",
    "# Helper Function:  Display top matches\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(image_scores, top_k=3):\n",
    "    top_images = sorted(image_scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    fig, axes = plt.subplots(1, top_k, figsize=(5 * top_k, 5))\n",
    "    if top_k == 1:\n",
    "        axes = [axes]\n",
    "    for ax, (img_path, score) in zip(axes, top_images):\n",
    "        ax.imshow(Image.open(img_path))\n",
    "        ax.set_title(f\"Score: {score:.2f}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625d965",
   "metadata": {},
   "source": [
    "## 🔟 Running an image search based on your inputs: \n",
    "\n",
    "**A look under the hood:** \n",
    "This is the core of the semantic image search loop.\n",
    "\n",
    "Here's what happens:\n",
    "1. Your input `caption` is turned into a vector (embedding) using OpenCLIP’s text encoder.\n",
    "2. Each image in the dataset is processed and converted to its own image embedding.\n",
    "3. The code calculates the similarity between the caption and each image using dot-product (cosine similarity under normalization).\n",
    "4. Each score tells us how well that image matches the meaning of your caption.\n",
    "5. We save those (image path, score) pairs to rank them later.\n",
    "\n",
    "Everything is done inside a `with torch.no_grad()` block so it runs efficiently and avoids memory buildup on GPU (if used).\n",
    "\n",
    "You can change the `caption` below to try new meme ideas and test the image retrieval accuracy as part of our testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Search: Static Caption\n",
    "# This is an example search that shows the top 3 images based on a static caption. It does not save the results for the next step.\n",
    "\n",
    "# Run a search based on text caption: \n",
    "\n",
    "caption = \"Trying to stay awake in class but the professor’s voice is a lullaby\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_tokens = tokenizer([caption]).to(device)\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    image_scores = []\n",
    "    for img_path in tqdm(image_paths):\n",
    "        image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (text_features @ image_features.T).item()\n",
    "        image_scores.append((img_path, similarity))\n",
    "\n",
    "# Show the top matches using the show_images function from above:\n",
    "# We're going to return the top 3 matches based on the similarity score.\n",
    "show_images(image_scores, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59eef39",
   "metadata": {},
   "source": [
    "## 1️⃣1️⃣ Run an image earch based on the caption you generated earlier: \n",
    "We are going to reference the `captions.json` created in the last notebook within this lab. \n",
    "\n",
    "This version lets you reuse captions generated earlier by loading them from a saved JSON file.\n",
    "Make sure `captions.json` exists (created from the previous `1_meme_generator_inst.ipynb` notebook.\n",
    "\n",
    "> If your captions.json is empty or showing captions different from your last run, go back to the first notebook, restart your kernel, and re run the cells to refresh the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cd9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Load captions.json generated in earlier notebook\n",
    "captions_file = Path(\"/content/MLLAcademy-2025/captions.json\")\n",
    "assert captions_file.exists(), \"captions.json not found. Run the Instructor Notebook to generate it first.\"\n",
    "\n",
    "with open(captions_file, \"r\") as f:\n",
    "    caption_options = json.load(f)\n",
    "\n",
    "caption_dropdown = widgets.Dropdown(\n",
    "    options=caption_options,\n",
    "    description='Caption:',\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Search Images\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        caption = caption_dropdown.value\n",
    "        print(f\"🔎 Searching for: {caption}\")\n",
    "\n",
    "        # Save the selected caption to ../selected_caption.json for use in Notebook B\n",
    "        with open(\"/content/MLLAcademy-2025/selected_caption.json\", \"w\") as f:\n",
    "            json.dump(caption, f)\n",
    "        print(f\"✅ Selected caption saved to '../selected_caption.json': {caption}\") # We'll use this .json file in the next notebook\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_tokens = tokenizer([caption]).to(device)\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            image_scores = []\n",
    "            for img_path in tqdm(image_paths):\n",
    "                image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "                image_features = model.encode_image(image)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                similarity = (text_features @ image_features.T).item()\n",
    "                image_scores.append((img_path, similarity))\n",
    "\n",
    "        show_images(image_scores, top_k=3)\n",
    "\n",
    "        # Sort and save top 3 images for next cell\n",
    "        image_scores_sorted = sorted(image_scores, key=lambda x: x[1], reverse=True)\n",
    "        top_image_paths = [str(path) for path, _ in image_scores_sorted[:3]]\n",
    "\n",
    "        with open(\"/content/MLLAcademy-2025/top_images.json\", \"w\") as f:\n",
    "            json.dump(top_image_paths, f)\n",
    "\n",
    "        print(\"✅ Top 3 image paths saved to '/content/MLLAcademy-2025/top_images.json' for selection in the next cell.\")\n",
    "\n",
    "run_button.on_click(on_click)\n",
    "display(caption_dropdown, run_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6c02f",
   "metadata": {},
   "source": [
    "## 1️⃣2️⃣ Select the image for your meme from results\n",
    "\n",
    "The cell below lets you select the final image from the top 3 matches and prepares it for use in the final meme.\n",
    "You'll choose one of the top image paths to pair with your caption selected from earlier.\n",
    "\n",
    "Run the cell below to select your image.\n",
    "\n",
    "**How to Use This Cell**:\n",
    "- Below, you'll see the 3 best image matches based on your selected meme text.\n",
    "- Each image has a **'Select This Image'** button beneath it.\n",
    "- Click the button under the image you like the most.\n",
    "\n",
    "\n",
    "Your selection will be saved to `/content/MLLAcademy-2025/selected_image.json` for the next part of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a938685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell lets you select the final image from the top 3 matches by clicking on the image.\n",
    "# --- Your selection will be saved for use in the final meme. --- \n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# Load top 3 image paths from generated JSON (instead of static list)\n",
    "with open(\"/content/MLLAcademy-2025/top_images.json\", \"r\") as f:\n",
    "    top_image_paths = json.load(f)\n",
    "\n",
    "# Prepare image widgets\n",
    "def image_to_widget(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    buffer = BytesIO()\n",
    "    img.thumbnail((400, 400))\n",
    "    img.save(buffer, format=\"JPEG\")\n",
    "    encoded = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return widgets.HTML(f'<img src=\"data:image/jpeg;base64,{encoded}\" style=\"border:2px solid black; margin:5px;\">')\n",
    "\n",
    "# Display images with buttons\n",
    "output = widgets.Output()\n",
    "\n",
    "selected_image = None\n",
    "\n",
    "def on_button_click(img_path):\n",
    "    global selected_image\n",
    "    selected_image = img_path\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(f\"✅ You selected: {img_path}\")\n",
    "        with open(\"/content/MLLAcademy/selected_image.json\", \"w\") as f:\n",
    "            json.dump(selected_image, f)\n",
    "        print(\"Saved selection to '/content/MLLAcademy/selected_image.json'.\")\n",
    "\n",
    "for img_path in top_image_paths:\n",
    "    img_widget = image_to_widget(img_path)\n",
    "    button = widgets.Button(description=\"Select This Image\", layout=widgets.Layout(width='auto'))\n",
    "    button.on_click(lambda b, p=img_path: on_button_click(p))\n",
    "    display(widgets.VBox([img_widget, button]))\n",
    "\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8951b09",
   "metadata": {},
   "source": [
    "## What Happens Next: 📝\n",
    "\n",
    "Once you've selected an image, it will be saved for you.\n",
    "\n",
    "You'll combine it with your chosen caption in the final lab step to create your meme!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50d1bc",
   "metadata": {},
   "source": [
    "## Checkpoint Complete! Image Ranking with OpenCLIP ✅\n",
    "\n",
    "You’ve now tested text-to-image semantic matching using OpenCLIP! Congratulations! \n",
    "\n",
    "You've completed\n",
    "- Entering a meme caption, loaded from your `captions.json` file.\n",
    "- Performed a vector embedding and search of your caption text using OpenCLIP to return the top 3 images based on that caption.\n",
    "- Displayed the scoring (confidence) of each image.\n",
    "- Picked an image you want to pair with the caption - these are the building blocks of your AI-meme.\n",
    "\n",
    "#### Next Steps:\n",
    "**→ You are now ready to integrate this into the a final product in the next notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729bc02",
   "metadata": {},
   "source": [
    "# 1️⃣3️⃣ Final Meme Assembly! 🏗️\n",
    "\n",
    "Welcome to the final part of your Meme Generator lab! 🎉\n",
    "\n",
    "In this notebook, you'll combine your selected image with your meme text to create the final meme image. We'll bring together the text caption you generated using an LLM, and the image you selected after searching OpenCLIP for the most suitable match for that caption.\n",
    "\n",
    "Let's get started by importing the necessary libraries we'll use in this notebook to bring it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run this cell ---\n",
    "\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975d45b",
   "metadata": {},
   "source": [
    "## 1️⃣4️⃣ Load your selected caption and image\n",
    "This next cell loads the selections you made in the previous parts of the lab:\n",
    "- Your chosen meme text from `captions.json`\n",
    "- The image path you selected from `selected_image.json`\n",
    "\n",
    "After you run this cell, you'll be presented with the \"selected caption\" and the \"final meme image\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load caption from the \"Selected Caption\" file we created earlier\n",
    "\n",
    "with open(\"/content/MLLAcademy-2025/selected_caption.json\", \"r\") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "# Get the first (and only) caption from the file:\n",
    "selected_caption = caption\n",
    "\n",
    "print(f\"✅ Your selected meme caption: {selected_caption}\\n\")\n",
    "\n",
    "# Load selected image\n",
    "with open(\"/content/MLLAcademy-2025/selected_image.json\", \"r\") as f:\n",
    "    selected_image_path = json.load(f)\n",
    "\n",
    "print(f\"✅ Selected image loaded from: {selected_image_path}\")\n",
    "\n",
    "# Display selected image\n",
    "from PIL import Image\n",
    "img = Image.open(selected_image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee10701",
   "metadata": {},
   "source": [
    "## 1️⃣5️⃣ Generate the final \"Meme Product\"\n",
    "\n",
    "This next cell overlays the selected caption \"meme text\" onto your chosen image. \n",
    "- You can customize the font size and position below w/in this cell using the various code snippets; only uncomment 1 at a time to move the text around. You can do this by adding or removing the `#`'s.\n",
    "\n",
    "**In this lab:**\n",
    "- We've added a dynamic font sizing system that scales text based on the image size.\n",
    "- If **Impact.ttf** is missing, we fall back to the default font.\n",
    "- You can change the font file and size in the `create_meme()` function!\n",
    "\n",
    "#### Example: How is the text centered on the image?\n",
    "\n",
    "We calculate the leftover space on each axis by subtracting the text size from the image size, then divide by 2. \n",
    "\n",
    "For example, the code:\n",
    "\n",
    "    `position = ((image.width - text_width) // 2, (image.height - text_height) // 2)`\n",
    "\n",
    "finds the horizontal (x) and vertical (y) center points. This is a common approach in pixel-based graphics.\n",
    "\n",
    "Once you've adjusted the cell's code - run the cell, and then click `Generate` within the widget to display your meme. This will also save a copy of your meme to `my_final_meme.png`. \n",
    "\n",
    "#### Tips: \n",
    "- The variable `font_size` controls how big your text font is relative to the size of the image. Decreasing this value will decrease the size of your font; increasing this value will increase the size of your font. \n",
    "\n",
    "- Feel free to re-run this cell as many times as you need to in order to get the text exactly where you want it. \n",
    "\n",
    "- There is a function called `draw.multiline_text` in the cell below - this is used _both_ for the white text _and_ a black outline that helps contrast the text on the image and ensures it looks consistent with an expected \"meme style\". It's in this function you can adjust the \"left\", \"right\", or \"center\" justification of the text - or even the color of your text if you wish. It's setup with white text by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create meme and save to your working directory\n",
    "def create_meme(image_path, text, output_path=\"/content/MLLAcademy-2025/my_final_meme.png\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load Impact font with dynamic sizing\n",
    "    font_path = \"../fonts/impact.ttf\" # gotta have that classic meme font\n",
    "    font_size = int(image.height * 0.07) # Dynamic sizing option\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    except:\n",
    "        print(\"Impact font not found! Using default font instead.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Wrap text to fit within the image width\n",
    "    max_chars_per_line = int(image.width / (font_size * 0.5))  # Estimate based on font size\n",
    "    wrapped_text = textwrap.fill(text, width=max_chars_per_line)\n",
    "\n",
    "    # Measure wrapped text size\n",
    "    bbox = draw.textbbox((0, 0), wrapped_text, font=font)\n",
    "    text_width = bbox[2] - bbox[0]\n",
    "    text_height = bbox[3] - bbox[1]\n",
    "\n",
    "   # --- More examples for moving/resizing text: ---\n",
    "   #  \n",
    "    # Position text at center of image\n",
    "    # position = ((image.width - text_width) // 2, (image.height - text_height) // 2)\n",
    "\n",
    "    # Text position: centered at bottom\n",
    "    position = ((image.width - text_width) // 2, image.height - text_height - 20)\n",
    "\n",
    "    # Top center\n",
    "    # position = ((image.width - text_width) // 2, 20)\n",
    "\n",
    "    # Bottom right corner\n",
    "    # position = (image.width - text_width - 20, image.height - text_height - 20)\n",
    "\n",
    "    # Bottom left corner\n",
    "    #position = (20, image.height - text_height - 20)\n",
    "\n",
    "    # Top left corner\n",
    "    # position = (20, 20)\n",
    "\n",
    "    # Change font size (e.g., larger text)\n",
    "    # font = ImageFont.truetype(\"arial.ttf\", 48) # Set a fixed font size\n",
    "    # text_width, text_height = draw.textsize(text, font=font)\n",
    "    # position = ((image.width - text_width) // 2, image.height - text_height - 20)\n",
    "\n",
    "    # Draw outline for visibility\n",
    "    outline_color = \"black\"\n",
    "    for x in [-2, 0, 2]:\n",
    "        for y in [-2, 0, 2]:\n",
    "            draw.multiline_text((position[0] + x, position[1] + y), wrapped_text, font=font, fill=outline_color, align=\"center\")\n",
    "\n",
    "    # Main text\n",
    "    draw.multiline_text(position, wrapped_text, font=font, fill=\"white\", align=\"center\")\n",
    "\n",
    "    image.save(output_path)\n",
    "    print(f\"✅ Meme saved to {output_path}\")\n",
    "    display(image)\n",
    "\n",
    "# Button to generate meme\n",
    "generate_button = widgets.Button(description=\"** Create My Meme! **\", button_style='success', layout=widgets.Layout(width='100%'))\n",
    "output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_generate_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        create_meme(selected_image_path, selected_caption)\n",
    "\n",
    "\n",
    "generate_button.on_click(on_generate_click)\n",
    "display(generate_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd142bc2",
   "metadata": {},
   "source": [
    "# Congratulations! 🎉\n",
    "### You've completed the Meme Generator Lab!\n",
    "\n",
    "- Your meme is saved as 'my-final_meme.png`. Feel free to share it, and if you'd like to submit it to our team for display later in the week by uploading it to the form linked on the display in class. \n",
    "\n",
    "#### Upload your final meme image here: \n",
    "> https://forms.gle/84USg6AWfq8r8J3p8\n",
    "\n",
    "If you have time, feel free to run the notebooks again to create more memes if you would like. \n",
    "\n",
    "In order to do this, you'll need to start with the begining to first generate a meme caption from your text topic inputs. \n",
    "\n",
    "This will overwrite your previous `captions.json`, `selected_caption.json`, `top_images.json` and `selected_image.json` files within this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d8a75",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
